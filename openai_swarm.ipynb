{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load API key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key=os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent B stepping in,  \n",
      "To assist you right away,  \n",
      "How can I now help?\n"
     ]
    }
   ],
   "source": [
    "from swarm import Swarm, Agent\n",
    "\n",
    "client = Swarm()\n",
    "\n",
    "def transfer_to_agent_b():\n",
    "    return agent_b\n",
    "\n",
    "\n",
    "agent_a = Agent(\n",
    "    name=\"Agent A\",\n",
    "    instructions=\"You are a helpful agent.\",\n",
    "    functions=[transfer_to_agent_b],\n",
    ")\n",
    "\n",
    "agent_b = Agent(\n",
    "    name=\"Agent B\",\n",
    "    instructions=\"Only speak in Haikus.\",\n",
    ")\n",
    "\n",
    "response = client.run(\n",
    "    agent=agent_a,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"I want to talk to agent B.\"}],\n",
    ")\n",
    "\n",
    "print(response.messages[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tranfer work to corresponding agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales Agent\n"
     ]
    }
   ],
   "source": [
    "sales_agent = Agent(name=\"Sales Agent\")\n",
    "\n",
    "def transfer_to_sales():\n",
    "   return sales_agent\n",
    "\n",
    "agent = Agent(functions=[transfer_to_sales])\n",
    "\n",
    "response = client.run(agent, [{\"role\":\"user\", \"content\":\"Transfer me to sales.\"}])\n",
    "print(response.agent.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer work between agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To provide progress updates on a task at every step, I will use various agents to monitor and report on the task's status. Hereâ€™s how I'll proceed:\n",
      "\n",
      "1. **Task Performance**: The task will be carried out by a dedicated task performer agent. \n",
      "2. **Data Processing**: A data processor agent will handle any data-related tasks.\n",
      "3. **Monitoring**: A monitor agent will track the progress and provide updates.\n",
      "4. **Assistance**: A helper agent will assist with any additional needs or support throughout the process.\n",
      "\n",
      "Let me know the specific task you need help with, and I can set up the process to give you progress updates at every step.\n"
     ]
    }
   ],
   "source": [
    "from swarm import Swarm, Agent\n",
    "\n",
    "client = Swarm()\n",
    "task_performer = Agent(\n",
    "    name=\"Task Performer\",\n",
    "    instructions=\"Your job is to perform tasks efficiently.\"\n",
    ")\n",
    "\n",
    "data_processor = Agent(\n",
    "    name=\"Data Processor\",\n",
    "    instructions=\"You process and analyze data.\"\n",
    ")\n",
    "\n",
    "monitor_agent = Agent(\n",
    "    name=\"Monitor Agent\",\n",
    "    instructions=\"You monitor the progress of tasks and ensure that everything is on track.\"\n",
    ")\n",
    "\n",
    "helper_agent = Agent(\n",
    "    name=\"Helper Agent\",\n",
    "    instructions=\"You assist other agents by providing necessary tools or resources.\"\n",
    ")\n",
    "\n",
    "def transfer_to_data_processor():\n",
    "    return data_processor\n",
    "\n",
    "task_performer.functions.append(transfer_to_data_processor)\n",
    "\n",
    "def task_performer_behavior():\n",
    "    return {\"role\": \"system\", \"content\": \"Task completed. Passing to Data Processor...\"}\n",
    "\n",
    "def data_processor_behavior():\n",
    "    return {\"role\": \"system\", \"content\": \"Data processed and analyzed.\"}\n",
    "\n",
    "def monitor_agent_behavior():\n",
    "    return {\"role\": \"system\", \"content\": \"Monitoring task progress...\"}\n",
    "\n",
    "def helper_agent_behavior():\n",
    "    return {\"role\": \"system\", \"content\": \"Providing assistance to other agents.\"}\n",
    "\n",
    "task_performer.functions.append(task_performer_behavior)\n",
    "task_performer.functions.append(data_processor_behavior)\n",
    "task_performer.functions.append(monitor_agent_behavior)\n",
    "task_performer.functions.append(helper_agent_behavior)\n",
    "\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Can you give me the progress of the task in every step of the way.\"}]\n",
    "response = client.run(agent=task_performer, messages=messages)\n",
    "print(response.messages[-1][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've selected a data processor to handle the task. Please provide the data you'd like analyzed, and any specific instructions or questions you have about it.\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"Please process and analyze the data. choose suitable agent for this process.\"}]\n",
    "\n",
    "response = client.run(agent=task_performer, messages=messages)\n",
    "print(response.messages[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handoffs Between Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handing off to programming Agent\n",
      "Here's a simple Python program that sums the values of an array:\n",
      "\n",
      "```python\n",
      "def sum_array(arr):\n",
      "    return sum(arr)\n",
      "\n",
      "# Example usage\n",
      "array = [1, 2, 3, 4, 5]\n",
      "result = sum_array(array)\n",
      "print(f\"The sum of the array is: {result}\")\n",
      "```\n",
      "\n",
      "In this program, the `sum_array` function takes a list of numbers as input and returns the sum of those numbers using Python's built-in `sum` function. The example usage demonstrates how to call the function and print the result.\n"
     ]
    }
   ],
   "source": [
    "from swarm import Swarm, Agent\n",
    "\n",
    "\n",
    "def handoff_to_sale_agent():\n",
    "    print(\"Handing off to sale Agent\")\n",
    "    return sale_agent\n",
    "\n",
    "def handoff_to_programming_agent():\n",
    "    print(\"Handing off to programming Agent\")\n",
    "    return programming_agent\n",
    "\n",
    "programming_agent = Agent(\n",
    "    name=\"Programming Agent\",\n",
    "    instructions=\"You handle only programming queries.\",\n",
    "    functions=[handoff_to_sale_agent]\n",
    ")\n",
    "\n",
    "sale_agent = Agent(\n",
    "    name=\"Sale Agent\",\n",
    "    instructions=\"You handle only Sale related information.\",\n",
    "    functions=[handoff_to_programming_agent]\n",
    ")\n",
    "\n",
    "client = Swarm()\n",
    "\n",
    "# Test handoff by asking a math question to the weather agent\n",
    "messages = [{\"role\": \"user\", \"content\": \"Can you write a python program that sums the values of an array?\"}]\n",
    "handoff_response = client.run(agent=sale_agent, messages=messages)\n",
    "print(handoff_response.messages[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following code is referenced from FutureSmart.ai (https://blog.futuresmart.ai/openai-swarm-a-hands-on-introduction).\n",
    "\n",
    "What We Do in the Code:\n",
    "1. Create a RAG (Retrieval-Augmented Generation) agent:\n",
    "   The RAG agent retrieves relevant information from a PDF file and generates an answer based on the context.\n",
    "2. Create an SQL agent:\n",
    "    The SQL agent works with a sample database, chinook.db, and can generate SQL queries based on natural language input.\n",
    "3. Call each agent independently: Each agent (RAG and SQL) is called independently to perform its respective task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For RAG Agent\n",
    "1. load documents and splitted\n",
    "2. embedding & store vector store\n",
    "3. RAG agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11 documents from the folder.\n",
      "Split the documents into 77 chunks.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "import os\n",
    "\n",
    "def load_documents(folder_path: str) -> List[Document]:\n",
    "    documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if filename.endswith('.pdf'):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif filename.endswith('.docx'):\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file type: {filename}\")\n",
    "            continue\n",
    "        documents.extend(loader.load())\n",
    "    return documents\n",
    "\n",
    "# Load documents from a folder\n",
    "folder_path = \"./pdf_file\"\n",
    "documents = load_documents(folder_path)\n",
    "print(f\"Loaded {len(documents)} documents from the folder.\")\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"Split the documents into {len(splits)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1423074/413957894.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "/home/test/anaconda3/envs/project/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created and persisted to './chroma_db'\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "collection_name = \"swarm_collection\"\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    collection_name=collection_name,\n",
    "    documents=splits,\n",
    "    embedding=embedding_function,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "print(\"Vector store created and persisted to './chroma_db'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating RAG agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling retrieve_and_generate\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Generative Agent-Based Models (GABMs) are a framework in which the agents do not make decisions about their interactions based on a fixed set of rules. Instead, a prompt is sent to a Large Language Model (LLM) with the desired details and it returns the decision that the agent should follow. The main idea behind GABMs is that the rules that agents have to follow are not completely fixed a priori. The decisions of the agent are driven by an LLM whose behavior can be enriched by prompting it with specific information about the problem, the social characteristics of the agent it should represent, or any other feature that is important for the model.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "def docs2str(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def retrieve_and_generate(question):\n",
    "    print(\"Calling retrieve_and_generate\")\n",
    "\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={'k': 5})\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    response = rag_chain.invoke(question)\n",
    "    return response\n",
    "\n",
    "# calling RAG agent directly\n",
    "retrieve_and_generate(\"what is generative agent based models?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SQLDatabase\n",
    "db = SQLDatabase.from_uri(\"sqlite:///chinook.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " def clean_sql_query(markdown_query):\n",
    "     # Split the query into lines\n",
    "     lines = markdown_query.strip().split('\\n')\n",
    "\n",
    "     # Remove markdown syntax lines\n",
    "     cleaned_lines = []\n",
    "     for line in lines:\n",
    "         # Skip lines that only contain backticks and optional language identifier\n",
    "         if line.strip().startswith('```') or line.strip() == 'sql':\n",
    "             continue\n",
    "         cleaned_lines.append(line)\n",
    "\n",
    "     # Join the remaining lines and clean up extra whitespace\n",
    "     cleaned_query = ' '.join(cleaned_lines).strip()\n",
    "\n",
    "     # Remove any remaining backticks\n",
    "     cleaned_query = cleaned_query.replace('`', '')\n",
    "\n",
    "     # Ensure semicolon at the end if not present\n",
    "     if not cleaned_query.strip().endswith(';'):\n",
    "         cleaned_query += ';'\n",
    "\n",
    "     return cleaned_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling sql_response_gen\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There are a total of 275 artists.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " from langchain_core.prompts import ChatPromptTemplate\n",
    " from langchain.chains import create_sql_query_chain\n",
    " from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    " from operator import itemgetter\n",
    " import re\n",
    " from langchain_core.output_parsers import StrOutputParser\n",
    " from langchain_core.prompts import PromptTemplate\n",
    " from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    " from langchain_openai import ChatOpenAI\n",
    "\n",
    " sql_prompt = ChatPromptTemplate.from_messages(\n",
    "     [\n",
    "         (\"system\", \"You are a SQLite expert expert. Given an input question, create a syntactically correct SQL query to run. Unless otherwise specificed.\\n\\nHere is the relevant table info: {table_info}\\n\\n Use max {top_k} rows\"),\n",
    "         (\"human\", \"{input}\"),\n",
    "     ]\n",
    " )\n",
    " llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    " def sql_response_gen(question):\n",
    "   print(\"Calling sql_response_gen\")\n",
    "   execute_query = QuerySQLDataBaseTool(db=db)\n",
    "   write_query = create_sql_query_chain(llm, db,sql_prompt)\n",
    "\n",
    "   answer_prompt = PromptTemplate.from_template(\n",
    "       \"\"\"Given the following user question, corresponding SQL query, and SQL result, answer the user question.\n",
    "\n",
    "   Question: {question}\n",
    "   SQL Query: {query}\n",
    "   SQL Result: {result}\n",
    "   Answer: \"\"\"\n",
    "   )\n",
    "\n",
    "   chain = (\n",
    "       RunnablePassthrough.assign(query=write_query | RunnableLambda(clean_sql_query)).assign(\n",
    "           result=itemgetter(\"query\") | execute_query\n",
    "       )\n",
    "       | answer_prompt\n",
    "       | llm\n",
    "       | StrOutputParser()\n",
    "   )\n",
    "\n",
    "   response = chain.invoke({\"question\": question})\n",
    "   return response\n",
    "\n",
    "#Calling sql agent directly \n",
    " sql_response_gen(\"give me the total number of artist?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SWARM - Multi Agent Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 1: Asking about the provided context ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[{'content': 'LLM stands for \"Large Language Model.\" It refers to a type of artificial intelligence model that is designed to understand, generate, and process natural language. These models are trained on vast amounts of text data and use deep learning techniques to learn patterns and meanings within the language. Examples of large language models include OpenAI\\'s GPT (Generative Pre-trained Transformer) series, BERT by Google, and others. These models are used for a variety of tasks, such as text completion, translation, summarization, and answering questions.', 'refusal': None, 'role': 'assistant', 'audio': None, 'function_call': None, 'tool_calls': None, 'sender': 'Main Agent'}] agent=Agent(name='Main Agent', model='gpt-4o', instructions='You are a helpful agent.', functions=[<function rag_agent_behavior at 0x7f28be0c4ae0>, <function nl2sql_agent_behavior at 0x7f28be0c4b80>], tool_choice=None, parallel_tool_calls=True) context_variables={}\n",
      "\n",
      "--- Example 2: Asking from the SQL DB ---\n",
      "messages=[{'content': 'Could you please specify the context or where this \"total number of artists table\" is located? Are you referring to a database or a specific dataset?', 'refusal': None, 'role': 'assistant', 'audio': None, 'function_call': None, 'tool_calls': None, 'sender': 'Main Agent'}] agent=Agent(name='Main Agent', model='gpt-4o', instructions='You are a helpful agent.', functions=[<function rag_agent_behavior at 0x7f28be0c4ae0>, <function nl2sql_agent_behavior at 0x7f28be0c4b80>], tool_choice=None, parallel_tool_calls=True) context_variables={}\n"
     ]
    }
   ],
   "source": [
    "from swarm import Swarm, Agent\n",
    "import re\n",
    "\n",
    "\n",
    "client = Swarm()\n",
    "\n",
    "main_agent = Agent(\n",
    "    name=\"Main Agent\",\n",
    "    instruction=\"Your job is to route the corresponding agent depending on the input task.\"\n",
    ")\n",
    "\n",
    "rag_agent = Agent(\n",
    "    name=\"RAG Agent\",\n",
    "    instructions=\"Your job is to retrieve relevant documents and generate answers using the context.\"\n",
    ")\n",
    "\n",
    "\n",
    "nl2sql_agent = Agent(\n",
    "    name=\"NL2SQL Agent\",\n",
    "    instructions=\"Your job is to convert natural language questions/queries into SQL queries using the provided database.\"\n",
    ")\n",
    "\n",
    "def rag_agent_behavior(messages):\n",
    "    question = messages[0][\"content\"]\n",
    "    return retrieve_and_generate(question)\n",
    "\n",
    "def nl2sql_agent_behavior(messages):\n",
    "    question = messages\n",
    "    return sql_response_gen(question)\n",
    "\n",
    "\n",
    "main_agent.functions.append(rag_agent_behavior)\n",
    "main_agent.functions.append(nl2sql_agent_behavior)\n",
    "\n",
    "print(\"\\n--- Example 1: Asking about the provided context ---\")\n",
    "messages = [{\"role\": \"user\", \"content\": \"What does LLM mean?\"}]\n",
    "response = client.run(agent=main_agent, messages=messages)\n",
    "if isinstance(response, Agent):\n",
    "    selected_agent = response\n",
    "    result = selected_agent.functions\n",
    "    print(result)\n",
    "else:\n",
    "    print(response)\n",
    "\n",
    "# --- Example 2: Asking from the SQL DB ---\n",
    "print(\"\\n--- Example 2: Asking from the SQL DB ---\")\n",
    "messages = [{\"role\": \"user\", \"content\": \"give me the total number of artists table?\"}]\n",
    "response = client.run(agent=main_agent, messages=messages)\n",
    "if isinstance(response, Agent):\n",
    "    selected_agent = response\n",
    "    result = selected_agent.functions\n",
    "    print(result)\n",
    "else:\n",
    "    print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectkernel",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
